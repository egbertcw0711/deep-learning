{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# download MNIST and read the data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (55000, 784) \n",
      "Test: (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", mnist.train.images.shape, \"\\nTest:\",mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a model\n",
    "import tensorflow as tf\n",
    "\n",
    "# computation graph\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # None means a dimension can be of any length\n",
    "y_tar = tf.placeholder(tf.float32, [None ,10])\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# loss function\n",
    "# A straight way of doing it (not numeric stable):\n",
    "# y = tf.nn.softmax(logits)\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tar*tf.log(y), reduction_indices=[1]))\n",
    "logits = tf.matmul(x,W) + b\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = y_tar, logits = logits)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training, the smaller the error margin, the better the model is\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# run the training step 1000 times!\n",
    "for _ in range(1000):\n",
    "    # stochastic training\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x:batch_xs, y_tar:batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8883\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y_tar,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# x, y are tf.placeholders\n",
    "print(sess.run(accuracy, feed_dict={x:mnist.test.images, y_tar:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1\n",
      "step 100, training accuracy 0.78\n",
      "step 200, training accuracy 0.94\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.9\n",
      "step 600, training accuracy 0.96\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.94\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 1\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 0.98\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 1\n",
      "step 1700, training accuracy 0.96\n",
      "step 1800, training accuracy 1\n",
      "step 1900, training accuracy 0.96\n",
      "test accuracy 0.982\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1,shape=shape))\n",
    "\n",
    "# tf.nn.conv2d(input, filter/weight/kernel,strides,padding)\n",
    "# input: [batch, in_height, in_width, in_channels]\n",
    "# filter: [height, width, in_channels, out_channels]\n",
    "# default stride: [batch,height,width,channels]\n",
    "# padding: 'SAME' / 'VALID'\n",
    "W_conv2d_1 = weight_variable([5,5,1,32]) # filter1\n",
    "b_conv2d_1 = bias_variable([32])\n",
    "\n",
    "# x_image = tf.placeholder(tf.float32,shape=[None,28,28,1])\n",
    "x_image = tf.reshape(x,[-1,28,28,1])\n",
    "z_conv2d_1 = tf.nn.conv2d(x_image,W_conv2d_1, strides=[1,1,1,1],padding='SAME')\n",
    "h_conv2d_1 = tf.nn.relu(z_conv2d_1)\n",
    "h_maxpool_1 = tf.nn.max_pool(h_conv2d_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "\n",
    "W_conv2d_2 = weight_variable([5,5,32,64])\n",
    "b_conv2d_2 = bias_variable([64])\n",
    "\n",
    "z_conv2d_2 = tf.nn.conv2d(h_maxpool_1,W_conv2d_2,strides=[1,1,1,1],padding='SAME')\n",
    "h_conv2d_2 = tf.nn.relu(z_conv2d_2)\n",
    "h_maxpool_2 = tf.nn.max_pool(h_conv2d_2, ksize=[1,2,2,1], strides=[1,2,2,1],padding = 'SAME')\n",
    "\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_maxpool_2_flat = tf.reshape(h_maxpool_2,[-1,7*7*64])\n",
    "z_fc1 = tf.matmul(h_maxpool_2_flat, W_fc1) + b_fc1\n",
    "h_fc1 = tf.nn.relu(z_fc1)\n",
    "\n",
    "# dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# output layer: softmax\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_out = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_tar, logits=y_out)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_out,1), tf.argmax(y_tar,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0],y_tar:batch[1],keep_prob:1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x:batch[0],y_tar:batch[1], keep_prob:0.5})\n",
    "    \n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict = {\n",
    "        x:mnist.test.images, y_tar:mnist.test.labels, keep_prob:1.0\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
